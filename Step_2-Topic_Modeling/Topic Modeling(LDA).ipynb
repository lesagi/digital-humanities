{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install python-bidi\n",
    "!pip install gensim\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import json \n",
    "import requests\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from bidi import algorithm as bidialg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SETTING PATHS FOR THIS PROJECT ####\n",
    "\n",
    "# Use this as base path if the project Files\n",
    "# and the Notebook located in the same directory\n",
    "base_path = pathlib.Path().absolute()\n",
    "\n",
    "# Setting the name and path to the Output folder\n",
    "output_path = os.path.join(base_path, \"output\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Setting the name and path to the Resources folder\n",
    "resources_path = os.path.join(base_path, \"resources\")\n",
    "if not os.path.exists(resources_path):\n",
    "    os.makedirs(resources_path)\n",
    "\n",
    "# functions below are for easing the access to files down the road\n",
    "def get_resource_file_path(filename):\n",
    "    return os.path.join(resources_path,\"{}\".format(filename))\n",
    "        \n",
    "def get_output_file_path(filename):\n",
    "    return os.path.join(output_path,\"{}\".format(filename))\n",
    "\n",
    "def get_file_content(file_path):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as infile:\n",
    "         return json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD RELEVANT FILES ####\n",
    "\n",
    "# Set Path to the lemmatized corpus and load to memory\n",
    "lemmatized_corpus_path = get_resource_file_path(\"laws_corpus_lemmatized.json\")\n",
    "lemmatized_corpus = get_file_content(lemmatized_corpus_path)\n",
    "\n",
    "# Set Path to laws index and load to memory\n",
    "laws_index_path = get_resource_file_path(\"laws_index.json\")\n",
    "laws_index = get_file_content(laws_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########      There is a file containing the word count for the lemmatized corpus       #########\n",
    "########           in the 'resources' folder, called 'laws_words_count.csv'             #########\n",
    "########               you should filter the stop words to your liking                  #########\n",
    "########   Open up a new text file, and add all stopwords to this file, line-by-line    #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After grouping all stop words to a single file\n",
    "# read the file to load the stopwords to memory\n",
    "# {filepath}: the path to the file with the stop words\n",
    "# @return: Set<string>\n",
    "def get_stopwords(filepath):\n",
    "    file = open(filepath, encoding='utf-8', mode='r')\n",
    "    lines = file.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    stopwords = set(lines)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the stopwords text file\n",
    "stopwords_path = get_resource_file_path(\"stopwords.txt\")\n",
    "\n",
    "# Loading stopwords to memory\n",
    "stopwords = get_stopwords(stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all laws and clean them for LDA\n",
    "# removing stopwords and blanks, and concatenating \n",
    "# all lists that make a law to one big list\n",
    "def get_cleaned_corpus(lemmatized_corpus):\n",
    "    clean_corpus = []\n",
    "    for document in lemmatized_corpus:\n",
    "        clean_law = [word for word in document if word is not \"\" and word not in stopwords]\n",
    "        if 0<len(clean_law):\n",
    "            clean_corpus.append(clean_law)\n",
    "        else:\n",
    "            print(document)\n",
    "    return clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a clean_corpus to be used for the model\n",
    "clean_corpus = get_cleaned_corpus(lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Preparing objects for gensim model #####\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(clean_corpus)\n",
    "\n",
    "# Create Corpus\n",
    "texts = clean_corpus\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# and extract to the project folder\n",
    "\n",
    "# Setting up the folder path and adding as env variable\n",
    "mallet_dir_path = get_resource_file_path(\"mallet-2.0.8\")\n",
    "os.environ['MALLET_HOME'] = mallet_dir_path\n",
    "\n",
    "# Setting up the path to the mallet folder\n",
    "mallet_path = os.path.join(mallet_dir_path,\"bin\",\"mallet\") # update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######    For optimized results, we need to check a various of topic divisions ######\n",
    "###### The compute_coherence_values function do just that, by generating the   ######\n",
    "###### LDA model iteratively for a wide range of topic count that can then be  ######\n",
    "######    used for comparison for picking the one with the highest score       ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, start=2, limit=70, step=2):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the model for a range of topic divisions\n",
    "\n",
    "# Defining the range\n",
    "start=24; limit=26; step=2;\n",
    "\n",
    "# Running the model for this range\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus, \n",
    "                                                        texts=lemmatized_corpus, \n",
    "                                                        start=start, \n",
    "                                                        limit=limit, \n",
    "                                                        step=step)\n",
    "\n",
    "# Showing the results for choosing the optimal model\n",
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose and Reference the optimal model in a variable\n",
    "optimal_model = model_list[0] # Replace 0 with the right index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LDA model to an external file\n",
    "with open(get_output_file_path('optimal_lda_model.pk'), 'wb') as lda_file:\n",
    "    pickle.dump(optimal_model, lda_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chosen LDA model if needed\n",
    "# CHANGE the path if needed\n",
    "with open(get_output_file_path('optimal_lda_model.pk'), 'rb') as lda_file:\n",
    "    optimal_model = pickle.load(lda_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace both values with the correspond values to the chosen model\n",
    "num_topics=24; num_words=40; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting topic words and put them in a list\n",
    "# topic string is something like that: \n",
    "# '0.35*\"one\" + 0.085*\"two\" + 0.002*\"three\" +...'\n",
    "# @return: ['one', 'two','three',...]\n",
    "def __extract_words_from_topic_string(string):\n",
    "    return [word for word in re.findall(\"\\w+\",string) if not re.match(\"\\d\", word)]\n",
    "\n",
    "def __get_words_per_topic(topics):\n",
    "    return [__extract_words_from_topic_string(topic[1]) for topic in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the topics list as strings\n",
    "model_topics = optimal_model.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "\n",
    "# extracting the words list for each topic\n",
    "topics_list = __get_words_per_topic(model_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics list to external CSV file\n",
    "# For assigning each topic a title more conviniently\n",
    "\n",
    "# {topics_path}: the path for the output file\n",
    "# {topic_words_list}: the lists of words for each topic\n",
    "# {num_words}: how many words from each topic to write\n",
    "# @return: None\n",
    "# @output: CSV file\n",
    "def export_topics_to_csv(topics_path, topic_words_list, num_words):\n",
    "    with open(topics_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for i in range(num_words):\n",
    "            writer.writerow([topic_words[i] for topic_words in topic_words_list])\n",
    "        f.close()\n",
    "            \n",
    "# path for the desired output\n",
    "topics_division_path = get_output_file_path(\"topics_divisions.csv\")\n",
    "\n",
    "# Export topics words to CSV file\n",
    "export_topics_to_csv(topics_division_path, topics_list, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######      This step is for associating each topic id      ######\n",
    "######           with a meaningful title             ######\n",
    "######     it is recommended doing it in excel and then\n",
    "######       copy the excel columns to a text file\n",
    "######           format is: <topic_title> \\t <id>\n",
    "######   an example can be found in the 'resource' folder   ######\n",
    "\n",
    "id2topic_path = get_resource_file_path('id2topic.txt')\n",
    "\n",
    "# LOAD the associated topic for each index from external file\n",
    "id2topic_file = open(id2topic_path, 'r', encoding='utf-8')\n",
    "id2topic_lines = id2topic_file.readlines()\n",
    "id2topic_file.close()\n",
    "id2topic = {}\n",
    "for line in id2topic_lines:\n",
    "    split = re.split(\"\\t\",line.strip())\n",
    "    if len(split) == 2:\n",
    "        [topic, topic_id] = split\n",
    "        id2topic[int(topic_id)-1] = topic.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __folder_path_to_date(date_str):\n",
    "    try:\n",
    "        x = re.search(\"\\d+-\\d+-\\d+\",date_str)\n",
    "        return x.group()\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for adding dates to each document, using\n",
    "# the laws_index which is an ordered list\n",
    "# of the path of each document in the 'akn' in \n",
    "# correspondence to each entry index in the DataFrame\n",
    "def __add_dates_to_df(dominant_topic_df, include_relative_path = False):\n",
    "    # Format to get Dominant Topic for Each\n",
    "    dominant_topic_df = dominant_topic_df.reset_index()\n",
    "    dominant_topic_df.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    dominant_topic_df['date'] = pd.Series(laws_index).map(__folder_path_to_date)\n",
    "    if include_relative_path:\n",
    "        dominant_topic_df['path'] = pd.Series(laws_index)\n",
    "    return dominant_topic_df\n",
    "\n",
    "# {include_topic_title} : adding the topic title to the DataFrame\n",
    "# {include_topic_title} : adding the relative path to law to the DataFrame\n",
    "# Based on the chosen model, it generates a table with  \n",
    "# the dominant topic of each document in the corpus\n",
    "def get_dominant_topic_df(ldamodel=optimal_model, corpus=corpus, texts=clean_corpus,\n",
    "                          include_topic_title=False, include_relative_path = False):\n",
    "    # Init output\n",
    "    dominant_topic_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                dominant_topic_df = dominant_topic_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    dominant_topic_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    dominant_topic_df = pd.concat([dominant_topic_df, contents], axis=1)\n",
    "    \n",
    "    # Add the dates to the DataFrame by the index (path in 'akn' folder)\n",
    "    dominant_topic_df = __add_dates_to_df(dominant_topic_df, include_relative_path)\n",
    "    \n",
    "    # Add the topic name to the DF\n",
    "    if include_topic_title:\n",
    "        dominant_topic_df['Topic'] = dominant_topic_df.apply (lambda row: id2topic[row['Dominant_Topic']], axis=1)\n",
    "    return dominant_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topic_df = get_dominant_topic_df(optimal_model, corpus, texts,include_topic_title=True, include_relative_path = True)\n",
    "dominant_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {df}: the output of 'get_dominant_topic_df'\n",
    "# {dates}: a list of tuples. each tuple\n",
    "# is (start_date, end_date), dates are\n",
    "# in format of yyyy-mm-dd\n",
    "# @return: dominant_topic_df table with\n",
    "# relevent dates only\n",
    "def __filter_dates(df, dates):    \n",
    "    filter_col = df['date'] == dates[0]\n",
    "    for date_range in dates:\n",
    "        start,end = date_range\n",
    "        filter_col = filter_col | (start <= df['date']) & (df['date'] <= end)\n",
    "    return df[filter_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging documents' topics for year 2012\n",
    "filtered_dominant_topic_df = __filter_dates(dominant_topic_df, [('2000-02-28', '2021-02-28')])\n",
    "filtered_dominant_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a DataFrame with stats of\n",
    "# documents count for each topic, and \n",
    "# their contribution to the mix in the corpus\n",
    "# IMPORTANT: it makes use of 'id2topic'\n",
    "\n",
    "# @input: a dominant_topic_df (filtered, or unfiltered)\n",
    "def get_topic_distribution(dominant_topic_df):\n",
    "    # Number of Documents for Each Topic\n",
    "    topics_count = dominant_topic_df['Dominant_Topic'].value_counts()\n",
    "    \n",
    "    # Percentage of Documents for Each Topic\n",
    "    topic_contribution = round(topics_count/topics_count.sum(), 4)\n",
    "\n",
    "    topic_distibution = pd.concat([topics_count, topic_contribution], axis=1)\n",
    "    topic_distibution = topic_distibution.reset_index()\n",
    "    topic_distibution.columns = ['Dominant_Topic','Num_Documents', 'Perc_Documents']\n",
    "\n",
    "    # Add the topic name to the DF\n",
    "    if len(topics_count) == 0:\n",
    "        topic_distibution['Topic'] = []\n",
    "    else:\n",
    "        topic_distibution['Topic'] = topic_distibution.apply (lambda row: id2topic[row['Dominant_Topic']], axis=1)\n",
    "        \n",
    "    return topic_distibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution_df = get_topic_distribution(filtered_dominant_topic_df)\n",
    "topic_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we'd want to add to the topics distribution, \n",
    "# topics that aren't present after filteration, we can use this function\n",
    "# it adds all topics (from id2topic) but leave their\n",
    "# contribution set to 0\n",
    "def __add_missing_topics(df):\n",
    "    existed_topic_ids = set(df['Dominant_Topic'].tolist())\n",
    "    for topic_id in id2topic.keys():\n",
    "        if topic_id not in existed_topic_ids:\n",
    "            df = df.append({'Dominant_Topic':topic_id,'Num_Documents':0, 'Perc_Documents':0,'Topic':u'{}'.format(id2topic[topic_id])}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dominant_topic_df table to CSV if needed\n",
    "dominant_topic_df.to_csv(get_output_file_path('dominant_topic_df.csv'), encoding='utf-8')\n",
    "\n",
    "# Saving the topic_distribution table to CSV if needed\n",
    "dominant_topic_df.to_csv(get_output_file_path('topic_distribution_df.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinig the filtering and distribution to one\n",
    "# easy-to-use function\n",
    "def get_topics_distribution_by_dates(dominant_topic_df, dates=[]):\n",
    "    \n",
    "    if len(dates) != 0:\n",
    "        dominant_topic_df = __filter_dates(dominant_topic_df, dates)\n",
    "\n",
    "    return get_topic_distribution(dominant_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same result as shown previously when ran\n",
    "# get_topic_distribution(filtered_dominant_topic_df)\n",
    "get_topics_distribution_by_dates(dominant_topic_df, [('2012-01-01','2013-01-01')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Wrapper function for topic_distribution_df\n",
    "# which gives you the option to choose if you want\n",
    "# to add missing_topics and filter by date\n",
    "# @return: DataFrame\n",
    "def get_topics_data_table(dominant_topic_df=dominant_topic_df, missing_topics=True, dates=[]):\n",
    "\n",
    "    topics_distribution = get_topics_distribution_by_dates(\n",
    "        dominant_topic_df, dates)\n",
    "\n",
    "    if missing_topics:\n",
    "        topics_distribution = __add_missing_topics(topics_distribution)\n",
    "\n",
    "    return topics_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics_data_table(dominant_topic_df=filtered_dominant_topic_df, missing_topics=True, dates=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Wrapper function for get_topics_data_table\n",
    "# that format the table as JSON object\n",
    "# @return JSON, a list, each row is an object\n",
    "def get_topics_data_json(dominant_topic_df=dominant_topic_df, missing_topics=True, dates=[]):\n",
    "    res = []\n",
    "    aggregated_dominant_topic_df = get_topics_data_table(\n",
    "        dominant_topic_df, missing_topics=missing_topics, dates=dates)\n",
    "    json_df = aggregated_dominant_topic_df.to_json(\n",
    "        orient='index', force_ascii=False)\n",
    "    json_df = json.loads(json_df)\n",
    "    for key in json_df:\n",
    "        res.append(json_df[key])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics_data_json(dominant_topic_df=filtered_dominant_topic_df, missing_topics=True, dates=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Horizontal Bar Chart to visualize the results\n",
    "# {df} : DataFrame of type topic_distribution\n",
    "# {ordered} : presenting the topics in descending order\n",
    "# {missing_topics} : add missing topics to the chart\n",
    "# @return: matplotlib plot\n",
    "def get_topics_distribution_chart(df, ordered = False, missing_topics=True):\n",
    "    # add missing values to DataFrame\n",
    "    if missing_topics:\n",
    "        df = __add_missing_topics(df)\n",
    "    \n",
    "    topics_count = df['Num_Documents'].tolist()\n",
    "    topic_names = df['Topic'].map(lambda x: bidialg.get_display(u'{}'.format(x))).tolist()\n",
    "    \n",
    "    if ordered:\n",
    "        topic_ids = list(range(0, len(topics_count)))\n",
    "    else:\n",
    "        topic_ids = df['Dominant_Topic'].tolist()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(topic_ids, topics_count, align='center')\n",
    "    ax.set_yticks(topic_ids)\n",
    "    ax.set_yticklabels(topic_names)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Performance')\n",
    "    ax.set_title('Topics Distribution')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_distribution_chart = get_topics_distribution_chart(topic_distribution_df, ordered = False, missing_topics=True)\n",
    "topics_distribution_chart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
